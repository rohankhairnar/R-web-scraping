{
    "collab_server" : "",
    "contents" : "library(xml2)\nlibrary(rvest)\nlibrary(utils)\n\nstart_time <- proc.time()\nlocal_file <- data.frame()\n\nsearch_term <- \"dell 8gb 500\"\nsearch_term_coded <- URLencode(search_term)\n\nurl1 <- \"https://www.amazon.com\"\nurl2 <- \"/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=\"\namazon_pd <- paste0(url1, url2, search_term_coded)\npage <- 1\ndeco0 <-\".............................................................................\"\ndeco <- \"***********************\"\n\nwhile(!is.null(amazon_pd))\n{\n  while(amazon_pd != \"https://www.amazon.com\")\n  {\n    writeLines(paste0(deco,(\" Web Scrapping: Page \"),page,(\" for \"),search_term,deco))\n    writeLines(paste0(\"Iteration \",page,\" initiated! \"))\n    amazon_html<- read_html(amazon_pd)\n    #write_html(amazon_html, file = \"/Users/temp/Desktop/R practise/amazhtml.txt\")\n    writeLines(\"Fetching attributes....\")\n    attributes<- html_attrs(html_nodes(amazon_html,\"#resultsCol .s-access-detail-page\"))\n    writeLines(\"Attributes fetched\")\n    \n    writeLines(paste0((\"Iteration \"),page,(\" begins:\"),deco0))\n    prod_titles <- sapply(attributes,'[[','title')\n    writeLines(paste0(\"Total products fetched: \", length(prod_titles)))\n    prod_links <- sapply(attributes,'[[','href')\n    \n    #building valid links for links missing domain. Some of the offer listed products have\n    #encoded urls, which need to be decoded and replaced\n    for(i in 1:length(prod_links))\n    {\n      if(length(prod_links[!grepl(url1,prod_links)]) != 0)\n      {\n        prod_links[i] <- gsub(\".*url=\",'',URLdecode(prod_links[i]))\n      }\n    }\n    \n    new_links <- prod_links\n    writeLines(paste0(length(new_links),\" links generated\"))\n\n    all_reviews_links <- gsub('/dp/','/product-reviews/',new_links)\n    writeLines(paste0(length(all_reviews_links), \" links for reviews generated\"))\n    \n    #fetching all product codes\n    prod_codes <- gsub('.*reviews/','',gsub('/ref.*','',all_reviews_links))\n    writeLines(paste0(length(prod_codes), \" product codes extracted\"))\n    \n    #parting the CSS Selectors in order to make it dynamic as per the product codes\n    part1 <- 'a.a-link-normal[href*='\n    part2 <- '] .sx-zero-spacing'\n    attributes_cost <- NULL\n    trial0 <- paste0(part1,prod_codes,part2)\n    prod_cost <- NULL\n    \n    for (i in 1:length(prod_codes))\n    {\n      if(length(html_nodes(amazon_html,trial0[i])) > 0)\n      {\n        attributes_cost[i] <-html_attrs(html_nodes(amazon_html,trial0[i]))\n        prod_cost[i] = sapply(attributes_cost[i],'[[','aria-label')\n      }\n      else\n      {\n        prod_cost[i] <- \"NA\"\n      }\n    }\n    \n    drop(part1)\n    drop(part2)\n    drop(trial0)\n    prod_cost = unlist(prod_cost)\n    writeLines(paste0(length(prod_cost), \" cost values fetched\"))\n    \n    \n    part1 <- \"span[name*='\"\n    part2 <- \"'] span.a-icon-alt\"\n    trial0 <- paste0(part1,prod_codes,part2)\n    prod_rating <- NULL\n    \n    for (i in 1:length(prod_codes))\n    {\n      if(length(html_nodes(amazon_html,trial0[i])) > 0)\n      {\n        prod_rating[i] <-html_text(html_nodes(amazon_html,trial0[i]))\n        prod_rating[i] <- trimws(gsub('out.*','',prod_rating[i]))\n      }\n      else\n      {\n        prod_rating[i] <- \"NA\"\n      }\n    }\n    drop(part1)\n    drop(part2)\n    drop(trial0)\n    writeLines(paste0(length(prod_rating),\" product ratings fecthed\"))\n    \n    #for number of reviewers\n    #html_text(html_nodes(amazon_html,\".a-row a-spacing-top-mini span[name='B00YVVE7YO'] .a-size-small\"))\n    \n    prod_data = data.frame(prod_titles,prod_cost,prod_rating,prod_codes,new_links,all_reviews_links)\n    \n    writeLines(\"Binding the collected data.....\")\n    local_file <- rbind(local_file,prod_data)\n    writeLines(\"Data sccessfully binded in local_file\")\n    writeLines(paste0(\"Iteration \",page,\" ends\"))\n    #for next iteration\n    next_pg <- html_attrs(html_nodes(amazon_html,\".pagnRA a\"))\n    next_pg_link <- paste0(url1,sapply(next_pg,'[[','href'))\n    #used in next iteration\n    amazon_pd <-next_pg_link\n    page = page +1\n  }\n  write.csv(local_file, file =\"amazonsearch.csv\")\n  amazon_pd <- NULL\n  writeLines(paste0(\"Process completed !!\"))\n  writeLines(paste0(\"Total products lined: \", length(local_file$prod_titles)))\n  writeLines(paste0(\"Total pages traversed: \", page))\n  writeLines(paste0(\"Results for \",search_term,\" saved at \",getwd(),\"/amazonsearch.csv\"))\n  writeLines(paste0(\"Process time: \"))\n  et <- proc.time()-start_time\n  print(et)\n}\n",
    "created" : 1491329827931.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "892742377",
    "id" : "587CD684",
    "lastKnownWriteTime" : 1491339444,
    "last_content_update" : 1491339444835,
    "path" : "~/R Projects/Web Scraping with R/amazonsearch.R",
    "project_path" : "amazonsearch.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}